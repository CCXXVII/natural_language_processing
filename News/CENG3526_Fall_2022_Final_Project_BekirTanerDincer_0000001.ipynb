{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAdFQpUUfT-d"
      },
      "source": [
        "# Final Project\n",
        "\n",
        "Genre detection is the process of identifying the genre or category of a piece of text, such as a book, movie, or piece of music. There are a few different approaches that can be used to perform genre detection, \n",
        "\n",
        "The general steps based on Vector Space model are as follows:\n",
        "\n",
        "1. Preprocess the text: The first step is to clean and prepare the text for analysis. This may involve removing punctuation, stop words, and other irrelevant information.\n",
        "\n",
        "2. Vectorize the text: Next, the text needs to be converted into a numerical representation that can be processed by a machine learning model. This is typically done using a technique called vectorization, which involves converting the text into a matrix of numbers.\n",
        "\n",
        "3. Train a machine learning model: Once the text has been vectorized, a machine learning model can be trained on a dataset of labeled examples to learn the relationship between the vectorized text and the corresponding genre labels.\n",
        "\n",
        "4. Use the trained model to make predictions: After the model has been trained, it can be used to make predictions on new, unseen text. The model will take the vectorized representation of the new text as input and output a predicted genre label.\n",
        "\n",
        "5. Evaluate the model: Finally, it is important to evaluate the performance of the model to ensure that it is making accurate predictions. This can be done by comparing the predicted labels to the true labels on a held-out test set.\n",
        "\n",
        "## For the language model, modify the Step-2 as follows:\n",
        "\n",
        "Instead of Vector Space model, use a language model. \n",
        "\n",
        "## For the Evaluation of each model use Cross Validation.\n",
        "\n",
        "In step 5 of the genre detection process, it is important to evaluate the performance of the machine learning model to ensure that it is making accurate predictions. One approach that can be used for this is cross-validation.\n",
        "\n",
        "Cross-validation is a method for evaluating the performance of a machine learning model by dividing the dataset into a training set and a test set, training the model on the training set, and evaluating its performance on the test set. This process is repeated a number of times, with different splits of the data, to get an average performance across all splits.\n",
        "\n",
        "To use cross-validation for evaluating a genre detection model, the first step is to split the dataset of labeled examples into a training set and a test set. The model can then be trained on the training set and its performance can be evaluated on the test set by comparing the predicted labels to the true labels. This process can be repeated a number of times with different splits of the data to get an average performance across all splits.\n",
        "\n",
        "The advantage of cross-validation is that it allows us to evaluate the model's performance on unseen data, which is more representative of how the model will perform on real-world data. This can help us to identify any overfitting or underfitting problems with the model and adjust the model accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "path = \"S:\\\\Downloads\\\\data sets\\\\42bin_haber\\\\42bin_haber\\\\42bin_haber\\\\news\"\n",
        "import os;\n",
        "\n",
        "# Genres\n",
        "dunya = []\n",
        "ekonomi = []\n",
        "genel = []\n",
        "guncel = []\n",
        "kultur = []\n",
        "magazin = []\n",
        "planet = []\n",
        "saglik = []\n",
        "siyaset = []\n",
        "spor = []\n",
        "teknoloji = []\n",
        "turkiye = []\n",
        "yasam = []\n",
        "\n",
        "\n",
        "for genre_name in os.listdir(path):\n",
        "    path_to_genre = path + \"\\\\\" + genre_name\n",
        "    for file_name in os.listdir(path_to_genre):\n",
        "        path_to_new = f\"{path_to_genre}/{file_name}\"\n",
        "        with open(path_to_new, \"rb\") as f:\n",
        "            new = f.read().decode(\"utf-8\").replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\".\",\"\").splitlines();\n",
        "            if genre_name == \"dunya\":\n",
        "                dunya.append(new)\n",
        "            elif genre_name == \"ekonomi\":\n",
        "                ekonomi.append(new)\n",
        "            elif genre_name == \"genel\":\n",
        "                genel.append(new)\n",
        "            elif genre_name == \"guncel\":\n",
        "                guncel.append(new)\n",
        "            elif genre_name == \"kultur\":\n",
        "                kultur.append(new)\n",
        "            elif genre_name == \"magazin\":\n",
        "                magazin.append(new)\n",
        "            elif genre_name == \"planet\":\n",
        "                planet.append(new)\n",
        "            elif genre_name == \"saglik\":\n",
        "                saglik.append(new)\n",
        "            elif genre_name == \"siyaset\":\n",
        "                siyaset.append(new)\n",
        "            elif genre_name == \"spor\":\n",
        "                spor.append(new)\n",
        "            elif genre_name == \"teknoloji\":\n",
        "                teknoloji.append(new)\n",
        "            elif genre_name == \"turkiye\":\n",
        "                turkiye.append(new)\n",
        "            elif genre_name == \"yasam\":\n",
        "                yasam.append(new)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Flatten all the lists\n",
        "flattened_dunya = [val for sublist in dunya for val in sublist]\n",
        "flattened_ekonomi = [val for sublist in ekonomi for val in sublist]\n",
        "flattened_genel = [val for sublist in genel for val in sublist]\n",
        "flattened_guncel = [val for sublist in guncel for val in sublist]\n",
        "flattened_kultur = [val for sublist in kultur for val in sublist]\n",
        "flattened_magazin = [val for sublist in magazin for val in sublist]\n",
        "flattened_planet = [val for sublist in planet for val in sublist]\n",
        "flattened_saglik = [val for sublist in saglik for val in sublist]\n",
        "flattened_siyaset = [val for sublist in siyaset for val in sublist]\n",
        "flattened_spor = [val for sublist in spor for val in sublist]\n",
        "flattened_teknoloji = [val for sublist in teknoloji for val in sublist]\n",
        "flattened_turkiye = [val for sublist in turkiye for val in sublist]\n",
        "flattened_yasam = [val for sublist in yasam for val in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\enest\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import nltk;\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_size_dunya = len(flattened_dunya)\n",
        "data_size_ekonomi = len(flattened_ekonomi)\n",
        "data_size_genel = len(flattened_genel)\n",
        "data_size_guncel = len(flattened_guncel)\n",
        "data_size_kultur = len(flattened_kultur)\n",
        "data_size_magazin = len(flattened_magazin)\n",
        "data_size_planet = len(flattened_planet)\n",
        "data_size_saglik = len(flattened_saglik)\n",
        "data_size_siyaset = len(flattened_siyaset)\n",
        "data_size_spor = len(flattened_spor)\n",
        "data_size_teknoloji = len(flattened_teknoloji)\n",
        "data_size_turkiye = len(flattened_turkiye)\n",
        "data_size_yasam = len(flattened_yasam)\n",
        "\n",
        "test_size_dunya = round(data_size_dunya * 0.1);\n",
        "test_size_ekonomi = round(data_size_ekonomi * 0.1);\n",
        "test_size_genel = round(data_size_genel * 0.1);\n",
        "test_size_guncel = round(data_size_guncel * 0.1);\n",
        "test_size_kultur = round(data_size_kultur * 0.1);\n",
        "test_size_magazin = round(data_size_magazin * 0.1);\n",
        "test_size_planet = round(data_size_planet * 0.1);\n",
        "test_size_saglik = round(data_size_saglik * 0.1);\n",
        "test_size_siyaset = round(data_size_siyaset * 0.1);\n",
        "test_size_spor = round(data_size_spor * 0.1);\n",
        "test_size_teknoloji = round(data_size_teknoloji * 0.1);\n",
        "test_size_turkiye = round(data_size_turkiye * 0.1);\n",
        "test_size_yasam = round(data_size_yasam * 0.1);\n",
        "\n",
        "test_index_dunya = [(x * test_size_dunya, x* test_size_dunya) for x in range(10)]\n",
        "test_index_ekonomi = [(x * test_size_ekonomi, x* test_size_ekonomi) for x in range(10)]\n",
        "test_index_genel = [(x * test_size_genel, x* test_size_genel) for x in range(10)]\n",
        "test_index_guncel = [(x * test_size_guncel, x* test_size_guncel) for x in range(10)]\n",
        "test_index_kultur = [(x * test_size_kultur, x* test_size_kultur) for x in range(10)]\n",
        "test_index_magazin = [(x * test_size_magazin, x* test_size_magazin) for x in range(10)]\n",
        "test_index_planet = [(x * test_size_planet, x* test_size_planet) for x in range(10)]\n",
        "test_index_saglik = [(x * test_size_saglik, x* test_size_saglik) for x in range(10)]\n",
        "test_index_siyaset = [(x * test_size_siyaset, x* test_size_siyaset) for x in range(10)]\n",
        "test_index_spor = [(x * test_size_spor, x* test_size_spor) for x in range(10)]\n",
        "test_index_teknoloji = [(x * test_size_teknoloji, x* test_size_teknoloji) for x in range(10)]\n",
        "test_index_turkiye = [(x * test_size_turkiye, x* test_size_turkiye) for x in range(10)]\n",
        "test_index_yasam = [(x * test_size_yasam, x* test_size_yasam) for x in range(10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenization(news_dunya, news_ekonomi, news_genel, news_guncel, news_kultur, news_magazin, news_planet, news_saglik, news_siyaset, news_spor, news_teknoloji, news_turkiye, news_yasam):\n",
        "    news_dunya_tokens = [nltk.word_tokenize(news) for news in news_dunya]\n",
        "    news_ekonomi_tokens = [nltk.word_tokenize(news) for news in news_ekonomi]\n",
        "    news_genel_tokens = [nltk.word_tokenize(news) for news in news_genel]\n",
        "    news_guncel_tokens = [nltk.word_tokenize(news) for news in news_guncel]\n",
        "    news_kultur_tokens = [nltk.word_tokenize(news) for news in news_kultur]\n",
        "    news_magazin_tokens = [nltk.word_tokenize(news) for news in news_magazin]\n",
        "    news_planet_tokens = [nltk.word_tokenize(news) for news in news_planet]\n",
        "    news_saglik_tokens = [nltk.word_tokenize(news) for news in news_saglik]\n",
        "    news_siyaset_tokens = [nltk.word_tokenize(news) for news in news_siyaset]\n",
        "    news_spor_tokens = [nltk.word_tokenize(news) for news in news_spor]\n",
        "    news_teknoloji_tokens = [nltk.word_tokenize(news) for news in news_teknoloji]\n",
        "    news_turkiye_tokens = [nltk.word_tokenize(news) for news in news_turkiye]\n",
        "    news_yasam_tokens = [nltk.word_tokenize(news) for news in news_yasam]\n",
        "    \n",
        "    return news_dunya_tokens, news_ekonomi_tokens, news_genel_tokens, news_guncel_tokens, news_kultur_tokens, news_magazin_tokens, news_planet_tokens, news_saglik_tokens, news_siyaset_tokens, news_spor_tokens, news_teknoloji_tokens, news_turkiye_tokens, news_yasam_tokens\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenization2(news_dunya):\n",
        "    news_dunya_tokens = [nltk.word_tokenize(news) for news in news_dunya]\n",
        "    return news_dunya_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(data, split_indexes):\n",
        "    data_test = data[split_indexes[0]:split_indexes[1]];\n",
        "    data_train = data[:split_indexes[0]] + data[split_indexes[1]:len(data)];\n",
        "    return {\"test\" : data_test, \"train\" : data_train}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    #news_dunya_tokens, news_ekonomi_tokens, news_genel_tokens, news_guncel_tokens, news_kultur_tokens, news_magazin_tokens, news_planet_tokens, news_saglik_tokens, news_siyaset_tokens, news_spor_tokens, news_teknoloji_tokens, news_turkiye_tokens, news_yasam_tokens = tokenization(flattened_dunya, flattened_ekonomi, flattened_genel, flattened_guncel, flattened_kultur, flattened_magazin, flattened_planet, flattened_saglik, flattened_siyaset, flattened_spor, flattened_teknoloji, flattened_turkiye, flattened_yasam)\n",
        "\n",
        "    news_dunya_tokens = tokenization2(flattened_dunya)\n",
        "\n",
        "    splits = split_dataset(news_dunya_tokens, test_index_dunya[i]);\n",
        "    test = splits[\"test\"];\n",
        "    train = splits[\"train\"];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Slice the dataset for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Calculating the frequency of each word in each genre\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4e0347718afc69be7b1c23768afdbedb062dd9a48333758dd9b559c5248491fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
